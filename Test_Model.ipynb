{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30154fb-1525-41a0-b22f-e5a64f997c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.models import densenet121, resnet18,resnet50\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e7d214-7be6-49b8-8c20-a808f7458583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, train_test_split\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "# Set the display option to show the full content of each column\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889823e5-07b9-4c22-9d27-7b677121e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # this is to handle the error for running model on cpu \n",
    "#https://github.com/huggingface/transformers/blob/ad78d9597b224443e9fe65a94acc8c0bc48cd039/docs/source/en/troubleshooting.md?plain=1#L110\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a866f6d-a6a1-4b42-bf5c-7b45b3721265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create exp direct and weight \n",
    "Exp_dir = Path(r'C:\\Users\\irams\\_IramS\\_Iram_MKK\\Project_AAPMM\\Deep_learning\\Experiments\\Exp_1')\n",
    "if not os.path.exists(Exp_dir):\n",
    "    os.mkdir(Exp_dir)\n",
    "\n",
    "\n",
    "weights_path = os.path.join(Exp_dir,\"Weights\")\n",
    "if not os.path.exists(weights_path):\n",
    "    os.mkdir(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5408b1ea-b0f7-4b91-9afa-acaa3db4dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only plotting function to see if transformed tensors are correct\n",
    "\n",
    "def plot_image_from_tensor(tensor, transformation=None):\n",
    "    # Apply the transformation if provided\n",
    "    transformed_tensor = tensor if transformation is None else transformation(tensor)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array\n",
    "    image_np = transformed_tensor.permute(1, 2, 0).numpy()  # Assuming CHW format\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71421a8f-1cd9-4e96-bd9f-ef40004e030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class DenseNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1, dropout_prob=0.4):\n",
    "        super(DenseNetClassifier, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=7, stride=1, padding=1)\n",
    "        \n",
    "        # Load pre-trained DenseNet model without classifier\n",
    "        self.densenet = densenet121(pretrained=False)\n",
    "        del self.densenet.classifier  # Remove existing classifier\n",
    "        \n",
    "        # Define the custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Dropout(dropout_prob),\n",
    "            nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        # Forward pass through the DenseNet feature extractor\n",
    "        \n",
    "        x = self.densenet.features(x)\n",
    "        \n",
    "        #x = F.relu(x, inplace=True)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "       \n",
    "        # Forward pass through the custom classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # Apply sigmoid activation for binary classification\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "#https://www.kaggle.com/code/arnoldyanga/image-classification-using-pytorch\n",
    "# for building better classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5749d22c-d40e-49af-8b58-dafc25b5e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1, dropout_prob=0.4):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=7, stride=1, padding=1)\n",
    "        \n",
    "        # Load pre-trained ResNet18 model without classifier\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Identity()  # Remove existing fully connected layer\n",
    "        \n",
    "        # Define the custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Forward pass through the ResNet feature extractor\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Forward pass through the custom classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # Apply sigmoid activation for binary classification\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2d7ab3-7e9d-4bfe-a3b9-c91809db0031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irams\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\irams\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialized device as 'cuda' if available, else 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model\n",
    "cnn_model = DenseNetClassifier(num_classes=1, dropout_prob=0.4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd286c9-79e8-486c-8b51-f131b9d29a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# initialized device as 'cuda' if available, else 'cpu'\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n# Train the model\\ncnn_model = ResNetClassifier(num_classes=1, dropout_prob=0.4).to(device)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# initialized device as 'cuda' if available, else 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model\n",
    "cnn_model = ResNetClassifier(num_classes=1, dropout_prob=0.4).to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5c5182-671c-48ac-bc2d-e3f84b7093f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize(object):\n",
    "    def __call__(self, img):\n",
    "        # Check if input is a NumPy array or a PyTorch tensor\n",
    "        if isinstance(img, np.ndarray):\n",
    "            level = 40\n",
    "            window = 400\n",
    "            max_val = level + window / 2\n",
    "            min_val = level - window / 2\n",
    "            img = np.clip(img, min_val, max_val)\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "            img= img.astype(\"float32\")\n",
    "        elif torch.is_tensor(img):\n",
    "            level = 40\n",
    "            window = 400\n",
    "            max_val = level + window / 2\n",
    "            min_val = level - window / 2\n",
    "            img = torch.clamp(img, min_val, max_val)\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "            img= img.float()\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported input type. Input must be a NumPy array or a PyTorch tensor.\")\n",
    "        return img\n",
    "\n",
    "class CustomResize(object):\n",
    "    def __call__(self, img):\n",
    "        # Check if input is a NumPy array or a PyTorch tensor\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # Set the desired dimensions\n",
    "            desired_width = 224\n",
    "            desired_height = 224\n",
    "            # Resize the image\n",
    "            resized_img = cv2.resize(img, (desired_width, desired_height))\n",
    "        elif torch.is_tensor(img):\n",
    "            # Set the desired dimensions\n",
    "            desired_width = 224\n",
    "            desired_height = 224\n",
    "            # Resize the image\n",
    "            resized_img = torch.nn.functional.interpolate(\n",
    "                img.unsqueeze(0),  # Add batch and channel dimensions\n",
    "                size=(desired_height, desired_width),  # Desired size\n",
    "                mode='bilinear',  # Interpolation mode\n",
    "                align_corners=False\n",
    "            ).squeeze(0)  # Remove batch and channel dimensions\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported input type. Input must be a NumPy array or a PyTorch tensor.\")\n",
    "        return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71bd8d7-9e9a-4b9e-ac09-08736a7a32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomNormalize(object):\n",
    "    def __call__(self, img):\n",
    "        # Check if input is a NumPy array, PIL image, or a PyTorch tensor\n",
    "        if isinstance(img, np.ndarray):\n",
    "            level = 40\n",
    "            window = 400\n",
    "            max_val = level + window / 2\n",
    "            min_val = level - window / 2\n",
    "            img = np.clip(img, min_val, max_val)\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "            img = img.astype(\"float32\")\n",
    "        elif isinstance(img, Image.Image):  # Check if input is a PIL image\n",
    "            # Convert PIL image to NumPy array\n",
    "            img = np.array(img)\n",
    "            level = 40\n",
    "            window = 400\n",
    "            max_val = level + window / 2\n",
    "            min_val = level - window / 2\n",
    "            img = np.clip(img, min_val, max_val)\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "            img = img.astype(\"float32\")\n",
    "        elif torch.is_tensor(img):\n",
    "            level = 40\n",
    "            window = 400\n",
    "            max_val = level + window / 2\n",
    "            min_val = level - window / 2\n",
    "            img = torch.clamp(img, min_val, max_val)\n",
    "            img = (img - min_val) / (max_val - min_val)\n",
    "            img = img.float()\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported input type. Input must be a NumPy array, PIL image, or a PyTorch tensor.\")\n",
    "        return img\n",
    "\n",
    "class CustomResize(object):\n",
    "    def __call__(self, img):\n",
    "        # Check if input is a NumPy array, PIL image, or a PyTorch tensor\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # Set the desired dimensions\n",
    "            desired_width = 224\n",
    "            desired_height = 224\n",
    "            # Resize the image\n",
    "            resized_img = cv2.resize(img, (desired_width, desired_height))\n",
    "        elif isinstance(img, Image.Image):  # Check if input is a PIL image\n",
    "            # Resize the PIL image\n",
    "            desired_width = 224\n",
    "            desired_height = 224\n",
    "            resized_img = img.resize((desired_width, desired_height), Image.BILINEAR)\n",
    "        elif torch.is_tensor(img):\n",
    "            # Set the desired dimensions\n",
    "            desired_width = 224\n",
    "            desired_height = 224\n",
    "            # Resize the tensor\n",
    "            resized_img = torch.nn.functional.interpolate(\n",
    "                img.unsqueeze(0),  # Add batch and channel dimensions\n",
    "                size=(desired_height, desired_width),  # Desired size\n",
    "                mode='bilinear',  # Interpolation mode\n",
    "                align_corners=False\n",
    "            ).squeeze(0)  # Remove batch and channel dimensions\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported input type. Input must be a NumPy array, PIL image, or a PyTorch tensor.\")\n",
    "        return resized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e68e79-426a-46b6-ae1b-84c315c7b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms.Compose sequence\n",
    "initial_transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert image to PyTorch tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8706909e-db69-4145-876e-3165a6147aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorch_data(Dataset):\n",
    "    def __init__(self, data_df, transform=None):\n",
    "        self.data_df = data_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        nifti_path = self.data_df.iloc[idx]['Paths']\n",
    "        nifti_img = sitk.ReadImage(nifti_path)  # Read file using SimpleITK\n",
    "        image_array = sitk.GetArrayFromImage(nifti_img)  # Convert to numpy array\n",
    "\n",
    "        # Convert numpy array to PIL Image\n",
    "        PIL_image = Image.fromarray(image_array)\n",
    "\n",
    "        label = self.data_df.iloc[idx]['Labels']\n",
    "        #subject_id = self.data_df.iloc[idx]['subject_id']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(PIL_image)\n",
    "            \n",
    "        return image, label \n",
    "\n",
    "        #return image, label, subject_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97960039-f5c8-47e7-9ebc-18aa6c0aac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the given dataset.\n",
    "    \n",
    "    Args:\n",
    "    - dataset: The dataset to create DataLoader from.\n",
    "    - batch_size: Batch size for DataLoader.\n",
    "    - is_training: Boolean flag indicating whether it's for training or validation.\n",
    "    \n",
    "    Returns:\n",
    "    - dataloader: The created DataLoader.\n",
    "    \"\"\"\n",
    "    # Choose shuffle option based on whether it's for training or validation\n",
    "    shuffle = is_training\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c1422c-64f5-43b1-82c1-c871e4759967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # Convert the input to a NumPy array if it's a tensor\n",
    "        if torch.is_tensor(tensor):\n",
    "            array = tensor.numpy()\n",
    "        elif isinstance(tensor, np.ndarray):\n",
    "            array = tensor\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported input type. Input must be a NumPy array or a PyTorch tensor.\")\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(self.mean, self.std, size=array.shape)\n",
    "        noisy_array = array + noise\n",
    "        # Clip the values to ensure they are within the valid range [0, 1]\n",
    "        noisy_array = np.clip(noisy_array, 0, 1)\n",
    "        \n",
    "        # Convert the noisy array back to a tensor if the input was a tensor\n",
    "        if torch.is_tensor(tensor):\n",
    "            noisy_tensor = torch.from_numpy(noisy_array)\n",
    "            return noisy_tensor\n",
    "        else:\n",
    "            return noisy_array\n",
    "\n",
    "\n",
    "class RandomGamma(object):\n",
    "    def __init__(self, gamma_range=(0.5, 2), gain=1):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.gain = gain\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gamma = random.uniform(self.gamma_range[0], self.gamma_range[1])\n",
    "        return transforms.functional.adjust_gamma(img, gamma, gain=self.gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fb66d9-e3f1-4b3f-ada3-39a0c1859f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_transf = transforms.Compose([\n",
    "        CustomNormalize(),  # Apply custom normalization\n",
    "        CustomResize(),\n",
    "        #transforms.RandomApply([transforms.RandomRotation(degrees=10)], p=0.2),\n",
    "        #transforms.RandomApply([transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(1.0, 1.0))], p=0.8),\n",
    "        #transforms.RandomApply([AddGaussianNoise(mean=0, std=np.random.uniform(0, 0.05))], p=0.5),\n",
    "        #transforms.RandomApply([RandomGamma(gamma_range=(0.5, 2))], p=0.2),\n",
    "        #transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.5, 1.5))], p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # For the validation dataset, we don't need any augmentation; simply convert images into tensors\n",
    "val_transf = transforms.Compose([\n",
    "        CustomNormalize(),  # Apply custom normalization\n",
    "        CustomResize(),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2604b0d3-6a9d-417b-b4ec-7f4da2deb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the loss value per batch of data\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target) # get loss\n",
    "    metric_b = (output.round() == target).float().mean()\n",
    "    \n",
    "    if opt is not None:\n",
    "\n",
    "        # backward pass\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # update weigths\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b.item()\n",
    "\n",
    "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
    "def loss_epoch(model,loss_func,dataset_dl,opt=None):\n",
    "    \n",
    "    run_loss=0.0 \n",
    "    t_metric=0.0\n",
    "    len_data=len(dataset_dl)\n",
    "  \n",
    "    # internal loop over dataset\n",
    "    for i, (xb, yb) in enumerate(dataset_dl):\n",
    "        # move batch to device\n",
    "\n",
    "        xb=xb.to(device)\n",
    "        \n",
    "        yb=yb.to(device)\n",
    "        \n",
    "        \n",
    "        yb=yb.view(-1, 1).float() # reshaping labels to match output of BCE loss function\n",
    "        \n",
    "        output=model(xb) # get model output on batch of the data \n",
    "        \n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt=None) # get loss per batch\n",
    "        \n",
    "        run_loss+=loss_b        # update running loss\n",
    "\n",
    "        if metric_b is not None: # update running metric\n",
    "            t_metric+=metric_b    \n",
    "    \n",
    "    loss=run_loss/float(len_data)  # average loss value\n",
    "    metric=t_metric/float(len_data) # average metric value\n",
    "    \n",
    "    return loss, metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90384f47-92ab-42c1-9c86-bba655cc84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, params, verbose=True):\n",
    "    \n",
    "    # Get the parameters\n",
    "    epochs=params[\"epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimiser\"]\n",
    "    train_dl=params[\"train_loader\"]\n",
    "    val_dl=params[\"val_loader\"]\n",
    "    weights_path=params[\"weights_path\"]\n",
    "    \n",
    "    loss_history={\"train_loss\": [],\"val_loss\": []} # history of loss values in each epoch\n",
    "    metric_history={\"train_acc\": [],\"val_acc\": []} # histroy of metric values in each epoch\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n",
    "    best_loss=float('inf') # initialize best loss to a large value\n",
    "    \n",
    "    ''' Train Model n_epochs '''\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        '''\n",
    "        Train Model Process\n",
    "        '''\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n",
    "\n",
    "        # collect losses\n",
    "        loss_history[\"train_loss\"].append(train_loss)\n",
    "        metric_history[\"train_acc\"].append(train_metric)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Evaluate Model Process\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n",
    "        \n",
    "        # store best model\n",
    "        if(val_loss < best_loss):\n",
    "            print('validation loss is less than best loss')\n",
    "            best_loss = val_loss\n",
    "            \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            weights_filename = f'weights.pt'\n",
    "            torch.save(model.state_dict(), os.path.join(weights_path,weights_filename))  \n",
    "            if(verbose):\n",
    "                print(\"Saved best model weights!\")\n",
    "\n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val_loss\"].append(val_loss)\n",
    "        metric_history[\"val_acc\"].append(val_metric)\n",
    "\n",
    "            \n",
    "        print(f\"Epoch:{epoch + 1} / {epochs},\"\n",
    "                  f\"train loss:{train_loss:.5f}, train acc: {train_metric:.5f}, \"\n",
    "                  f\"valid loss:{val_loss:.5f}, valid acc:{val_metric:.5f}\")\n",
    "        print(\"-\" * 100) \n",
    "\n",
    "\n",
    "    # load best weights to final model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "094d6b49-4d69-40f7-992e-5cc285b79ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for negative and positive directories\n",
    "sgmd_neg = Path(r\"C:\\Users\\irams\\_IramS\\_Iram_MKK\\Project_AAPMM\\Deep_learning\\Data\\Test_2\\normal\")\n",
    "sgmd_pos = Path(r\"C:\\Users\\irams\\_IramS\\_Iram_MKK\\Project_AAPMM\\Deep_learning\\Data\\Test_2\\sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd3c591e-387b-4626-862f-00a57685a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in negative and positive directories\n",
    "sgmd_neg_paths = []\n",
    "for root, dirs, files in os.walk(sgmd_neg):\n",
    "    for dir in dirs:\n",
    "        nifti_2d_folder = os.path.join(root, dir, 'Patch_2D')\n",
    "        if os.path.isdir(nifti_2d_folder):\n",
    "            for file in sorted(os.listdir(nifti_2d_folder), key=lambda x: os.path.getmtime(os.path.join(nifti_2d_folder, x))):\n",
    "                if file.endswith(\".nii\"):\n",
    "                    sgmd_neg_paths.append(os.path.join(nifti_2d_folder, file))\n",
    "\n",
    "sgmd_pos_paths = []\n",
    "for root, dirs, files in os.walk(sgmd_pos):\n",
    "    for dir in dirs:\n",
    "        nifti_2d_folder = os.path.join(root, dir, 'Patch_2D')\n",
    "        if os.path.isdir(nifti_2d_folder):\n",
    "            for file in sorted(os.listdir(nifti_2d_folder), key=lambda x: os.path.getmtime(os.path.join(nifti_2d_folder, x))):\n",
    "                if file.endswith(\".nii\"):\n",
    "                    sgmd_pos_paths.append(os.path.join(nifti_2d_folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af4eea5-940e-4b36-896b-8297f2925e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "sgmd_neg_labels = [0] * len(sgmd_neg_paths)\n",
    "sgmd_pos_labels = [1] * len(sgmd_pos_paths)\n",
    "\n",
    "# Concatenate labels\n",
    "labels = pd.DataFrame({'Labels': sgmd_neg_labels + sgmd_pos_labels})\n",
    "\n",
    "# Extract subject IDs or file paths\n",
    "subject_ids = [path.split('Patch_2D')[0].split(os.path.sep)[-2] for path in sgmd_neg_paths + sgmd_pos_paths]\n",
    "\n",
    "\n",
    "# Create DataFrame with subject IDs , labels and path to all images of each subject\n",
    "data_set = pd.DataFrame({'subject_id': subject_ids, 'Labels': labels['Labels'], 'Paths': sgmd_neg_paths + sgmd_pos_paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcc53727-462f-418f-9f7e-e5a97a23fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on subject_id, this dataframe can be then used for stratified splitting based on uniques \n",
    "# subject ids\n",
    "data_subject = data_set.drop_duplicates(subset=['subject_id'])\n",
    "\n",
    "# Drop the 'Paths' column\n",
    "data_subject = data_subject.drop(columns=['Paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e166b52-d0ab-4e5e-9d4e-39ae64520ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified splitting\n",
    "# full indicates entire training data that will be further spilt into train and validation in next step\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data_subject['subject_id'], data_subject['Labels'], test_size=0.2, stratify=data_subject['Labels'], random_state=42)\n",
    "\n",
    "# Create dataframes for train and test sets\n",
    "train_df = pd.DataFrame({'subject_id': X_train, 'Labels': y_train})\n",
    "valid_df = pd.DataFrame({'subject_id': X_valid, 'Labels': y_valid})\n",
    "\n",
    "# Calculate class distribution in train and test sets\n",
    "train_class_distribution_full = train_df['Labels'].value_counts(normalize=True)\n",
    "valid_class_distribution = valid_df['Labels'].value_counts(normalize=True)\n",
    "\n",
    "# Print class distribution\n",
    "#print(\"Train class distribution:\")\n",
    "#print(train_class_distribution_full)\n",
    "#print(\"\\nValid class distribution:\")\n",
    "#print(valid_class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6605ac72-8b53-452a-8b18-e494cb1cfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows from dataset where subject_id matches those in X_train and X_val\n",
    "X_train_ = data_set[data_set['subject_id'].isin(train_df['subject_id'])]\n",
    "X_val_ = data_set[data_set['subject_id'].isin(valid_df['subject_id'])]\n",
    "    \n",
    "# Reste indexes\n",
    "X_train_.reset_index(drop=True, inplace=True)\n",
    "X_val_.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = pytorch_data(data_df=X_train_, transform=initial_transform)\n",
    "val_dataset = pytorch_data(data_df=X_val_, transform=initial_transform)\n",
    "    \n",
    "# Apply augmentation \n",
    "train_dataset.transform = tr_transf\n",
    "val_dataset.transform = val_transf\n",
    "    \n",
    "    #original_image, label = train_dataset[15]\n",
    "    #print(original_image.shape)\n",
    "    # Plot the original image\n",
    "    #plot_image_from_tensor(original_image)\n",
    "    \n",
    "train_dl = create_dataloader(train_dataset, batch_size=32, is_training=True)\n",
    "val_dl = create_dataloader(val_dataset, batch_size=32, is_training=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba3fad-1843-4c58-89b2-15a6add2e27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b3ab094b924fbd97c372b24f332292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:1 / 30,train loss:0.67338, train acc: 0.70602, valid loss:0.67954, valid acc:0.59375\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:2 / 30,train loss:0.67424, train acc: 0.70516, valid loss:0.67808, valid acc:0.55208\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:3 / 30,train loss:0.67412, train acc: 0.71633, valid loss:0.67456, valid acc:0.57292\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:4 / 30,train loss:0.67564, train acc: 0.71602, valid loss:0.67400, valid acc:0.55729\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:5 / 30,train loss:0.67358, train acc: 0.71352, valid loss:0.67379, valid acc:0.55729\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:6 / 30,train loss:0.67457, train acc: 0.72328, valid loss:0.67362, valid acc:0.57396\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:7 / 30,train loss:0.67417, train acc: 0.73336, valid loss:0.67363, valid acc:0.58542\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:8 / 30,train loss:0.67480, train acc: 0.70961, valid loss:0.67368, valid acc:0.55833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "validation loss is less than best loss\n",
      "Saved best model weights!\n",
      "Epoch:9 / 30,train loss:0.67694, train acc: 0.68367, valid loss:0.67332, valid acc:0.58438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:10 / 30,train loss:0.67479, train acc: 0.69484, valid loss:0.67349, valid acc:0.58021\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:11 / 30,train loss:0.67384, train acc: 0.73023, valid loss:0.67362, valid acc:0.58542\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:12 / 30,train loss:0.67452, train acc: 0.70906, valid loss:0.67361, valid acc:0.56875\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:13 / 30,train loss:0.67354, train acc: 0.71078, valid loss:0.67348, valid acc:0.58542\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "params_train = {\n",
    "        \"train_loader\": train_dl,\n",
    "        \"val_loader\": val_dl,\n",
    "        \"epochs\": 30,\n",
    "        \"optimiser\":optim.Adam(cnn_model.parameters(), lr=2e-3, weight_decay=0.09),\n",
    "        \"loss_func\": nn.BCEWithLogitsLoss(),\n",
    "        \"weights_path\": weights_path,\n",
    "}\n",
    "    \n",
    "''' Actual Train / Evaluation of CNN Model '''\n",
    "# train and validate the model\n",
    "cnn_model, loss_history, metric_history = train_val(cnn_model, params_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e4766-a18f-4510-b47b-9f226be20f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Number of epochs\n",
    "epochs = params_train[\"epochs\"]\n",
    "\n",
    "# Initialize subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "sns.lineplot(x=[*range(1, epochs + 1)], y=loss_history[\"train_loss\"], ax=ax[0], label='Train Loss')\n",
    "sns.lineplot(x=[*range(1, epochs + 1)], y=loss_history[\"val_loss\"], ax=ax[0], label='Validation Loss')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "sns.lineplot(x=[*range(1, epochs + 1)], y=metric_history[\"train_acc\"], ax=ax[1], label='Train Accuracy')\n",
    "sns.lineplot(x=[*range(1, epochs + 1)], y=metric_history[\"val_acc\"], ax=ax[1], label='Validation Accuracy')\n",
    "\n",
    "# Set titles and labels\n",
    "ax[0].set_title(f'Loss History')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].set_title(f'Accuracy History')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5adb9-0deb-4dad-9cef-1ab44b901545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ebdc9-3779-4280-af04-7787bf37b21c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
